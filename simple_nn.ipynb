{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ankurzing/sentiment-analysis-for-financial-news?dataset_version_number=5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 903k/903k [00:00<00:00, 14.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /Users/marius/.cache/kagglehub/datasets/ankurzing/sentiment-analysis-for-financial-news/versions/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ankurzing/sentiment-analysis-for-financial-news\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/marius/.cache/kagglehub/datasets/ankurzing/sentiment-analysis-for-financial-news/versions/5\n",
      "Files in dataset directory: ['FinancialPhraseBank', 'all-data.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(\"Files in dataset directory:\", os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = os.path.join(path, \"all-data.csv\")\n",
    "\n",
    "columns = [\"Sentiment\", \"News Headline\"]\n",
    "\n",
    "full_data = pd.read_csv(file_path, encoding='latin-1', names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                               text\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "3  positive  With the new production plant the company woul...\n",
      "4  positive  According to the company 's updated strategy f...\n",
      "5  positive  FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...\n",
      "6  positive  For the last quarter of 2010 , Componenta 's n...\n",
      "7  positive  In the third quarter of 2010 , net sales incre...\n",
      "8  positive  Operating profit rose to EUR 13.1 mn from EUR ...\n",
      "9  positive  Operating profit totalled EUR 21.1 mn , up fro...\n"
     ]
    }
   ],
   "source": [
    "#we are interested in the labels and tweets only\n",
    "\n",
    "data = full_data[['Sentiment', 'News Headline']].copy()\n",
    "data.rename(columns={\"Sentiment\": \"label\"}, inplace=True)\n",
    "data.rename(columns={\"News Headline\": \"text\"}, inplace=True)\n",
    "\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Uncomment this if you want to run the preprocessing without parallelization!\\n\\ndef preprocess_text(text):\\n    # Tokenize text\\n    tokens = word_tokenize(text.lower())  # Lowercase and tokenize\\n\\n    # Remove punctuation and non-alphabetic characters\\n    tokens = [re.sub(r\"[^a-zA-Z]\", \"\", token) for token in tokens]\\n    tokens = [token for token in tokens if token]  # Remove empty strings\\n\\n    # Remove stopwords\\n    stop_words = set(stopwords.words(\\'english\\'))\\n    filtered_tokens = [word for word in tokens if word not in stop_words]\\n\\n    # Lemmatize tokens\\n    lemmatizer = WordNetLemmatizer()\\n    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\\n\\n    return \\' \\'.join(lemmatized_tokens) \\n\\ndata[\\'processed_text\\'] = data[\\'text\\'].apply(preprocess_text)\\nprint(data.head())\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing function without parallelization (on my hardware took ~3min)\n",
    "\n",
    "''' Uncomment this if you want to run the preprocessing without parallelization!\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
    "\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    tokens = [re.sub(r\"[^a-zA-Z]\", \"\", token) for token in tokens]\n",
    "    tokens = [token for token in tokens if token]  # Remove empty strings\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens) \n",
    "\n",
    "data['processed_text'] = data['text'].apply(preprocess_text)\n",
    "print(data.head())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function with parallelization (on my hardware took ~2min)\n",
    "# Roughly a 33% speed increase\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
    "\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    tokens = [re.sub(r\"[^a-zA-Z]\", \"\", token) for token in tokens]\n",
    "    tokens = [token for token in tokens if token]  # Remove empty strings\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)  # Return processed text as a single string\n",
    "\n",
    "def preprocess_and_return(row):\n",
    "    return preprocess_text(row['text'])\n",
    "\n",
    "data['processed_text'] = Parallel(n_jobs=-1)(delayed(preprocess_and_return)(row) for index, row in data.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n",
      "Processed Tweet: according gran company plan move production russia although company growing\n"
     ]
    }
   ],
   "source": [
    "first_tweet = data.iloc[0]\n",
    "print(\"Original Tweet:\", first_tweet['text'])\n",
    "print(\"Processed Tweet:\", first_tweet['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7250, Accuracy: 0.9948\n",
      "Epoch 2/10, Loss: 0.1144, Accuracy: 1.0000\n",
      "Epoch 3/10, Loss: 0.0228, Accuracy: 1.0000\n",
      "Epoch 4/10, Loss: 0.0084, Accuracy: 1.0000\n",
      "Epoch 5/10, Loss: 0.0037, Accuracy: 1.0000\n",
      "Epoch 6/10, Loss: 0.0019, Accuracy: 1.0000\n",
      "Epoch 7/10, Loss: 0.0011, Accuracy: 1.0000\n",
      "Epoch 8/10, Loss: 0.0007, Accuracy: 1.0000\n",
      "Epoch 9/10, Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch 10/10, Loss: 0.0004, Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class SentimentNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SentimentNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # Input to hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Hidden to output layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x  # Output logits for multi-class classification\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 50\n",
    "hidden_dim = 32\n",
    "output_dim = 3  # Output dimension is 3 for multi-class classification (negative, neutral, positive)\n",
    "\n",
    "# Assume 'data' is a DataFrame with 'text' and 'label' columns\n",
    "# Map string labels to numeric\n",
    "label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "data['label'] = data['label'].map(label_map)\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_data['text']).toarray()\n",
    "X_test_tfidf = vectorizer.transform(test_data['text']).toarray()\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train_tfidf, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_tfidf, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(train_data['label'].values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(test_data['label'].values, dtype=torch.long)\n",
    "\n",
    "# Model setup\n",
    "model = SentimentNN(input_dim=1000, hidden_dim=64, output_dim=3)  # Three output classes\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get predictions and compute accuracy\n",
    "        _, predicted = torch.max(outputs, 1)  # Get class with max probability\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct_predictions = (predicted == y_test_tensor).sum().item()\n",
    "    total_predictions = y_test_tensor.size(0)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(input_text, model, vectorizer):\n",
    "    # Preprocess input (apply same preprocessing as during training)\n",
    "    # If you had specific preprocessing (like lowercasing, removing stopwords, etc.), apply here\n",
    "    processed_text = input_text.lower()  # Example preprocessing step\n",
    "\n",
    "    # Convert text to TF-IDF features using the trained vectorizer\n",
    "    input_tfidf = vectorizer.transform([processed_text]).toarray()\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    input_tensor = torch.tensor(input_tfidf, dtype=torch.float32)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculation since we are not training\n",
    "    with torch.no_grad():\n",
    "        # Pass the input through the model\n",
    "        outputs = model(input_tensor)\n",
    "\n",
    "        # Get predicted class (index of the highest value)\n",
    "        _, predicted_class = torch.max(outputs, 1)\n",
    "\n",
    "        # Map predicted class back to original label\n",
    "        label_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "        predicted_label = label_mapping[predicted_class.item()]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage\n",
    "input_text = \"I don't really mind this, but usually I dont mind this\"\n",
    "predicted_sentiment = predict_sentiment(input_text, model, vectorizer)\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
