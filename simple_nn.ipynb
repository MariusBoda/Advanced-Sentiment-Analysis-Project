{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/marius/.cache/kagglehub/datasets/kazanova/sentiment140/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/marius/.cache/kagglehub/datasets/kazanova/sentiment140/versions/2\n",
      "Files in dataset directory: ['training.1600000.processed.noemoticon.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(\"Files in dataset directory:\", os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = os.path.join(path, \"training.1600000.processed.noemoticon.csv\")\n",
    "\n",
    "columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "full_data = pd.read_csv(file_path, encoding='latin-1', names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1      0  is upset that he can't update his Facebook by ...\n",
      "2      0  @Kenichan I dived many times for the ball. Man...\n",
      "3      0    my whole body feels itchy and like its on fire \n",
      "4      0  @nationwideclass no, it's not behaving at all....\n",
      "5      0                      @Kwesidei not the whole crew \n",
      "6      0                                        Need a hug \n",
      "7      0  @LOLTrish hey  long time no see! Yes.. Rains a...\n",
      "8      0               @Tatiana_K nope they didn't have it \n",
      "9      0                          @twittera que me muera ? \n"
     ]
    }
   ],
   "source": [
    "#we are interested in the labels and tweets only\n",
    "\n",
    "data = full_data[['target', 'text']].copy()\n",
    "data.rename(columns={\"target\": \"label\"}, inplace=True)\n",
    "\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Uncomment this if you want to run the preprocessing without parallelization!\\n\\ndef preprocess_text(text):\\n    # Tokenize text\\n    tokens = word_tokenize(text.lower())  # Lowercase and tokenize\\n\\n    # Remove punctuation and non-alphabetic characters\\n    tokens = [re.sub(r\"[^a-zA-Z]\", \"\", token) for token in tokens]\\n    tokens = [token for token in tokens if token]  # Remove empty strings\\n\\n    # Remove stopwords\\n    stop_words = set(stopwords.words(\\'english\\'))\\n    filtered_tokens = [word for word in tokens if word not in stop_words]\\n\\n    # Lemmatize tokens\\n    lemmatizer = WordNetLemmatizer()\\n    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\\n\\n    return \\' \\'.join(lemmatized_tokens) \\n\\ndata[\\'processed_text\\'] = data[\\'text\\'].apply(preprocess_text)\\nprint(data.head())\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing function without parallelization (on my hardware took ~3min)\n",
    "\n",
    "''' Uncomment this if you want to run the preprocessing without parallelization!\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
    "\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    tokens = [re.sub(r\"[^a-zA-Z]\", \"\", token) for token in tokens]\n",
    "    tokens = [token for token in tokens if token]  # Remove empty strings\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens) \n",
    "\n",
    "data['processed_text'] = data['text'].apply(preprocess_text)\n",
    "print(data.head())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function with parallelization (on my hardware took ~2min)\n",
    "# Roughly a 33% speed increase\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
    "\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    tokens = [re.sub(r\"[^a-zA-Z]\", \"\", token) for token in tokens]\n",
    "    tokens = [token for token in tokens if token]  # Remove empty strings\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)  # Return processed text as a single string\n",
    "\n",
    "def preprocess_and_return(row):\n",
    "    return preprocess_text(row['text'])\n",
    "\n",
    "data['processed_text'] = Parallel(n_jobs=-1)(delayed(preprocess_and_return)(row) for index, row in data.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "Processed Tweet: switchfoot http twitpiccomyzl awww bummer shoulda got david carr third day\n"
     ]
    }
   ],
   "source": [
    "first_tweet = data.iloc[0]\n",
    "print(\"Original Tweet:\", first_tweet['text'])\n",
    "print(\"Processed Tweet:\", first_tweet['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5050, Accuracy: 0.7510\n",
      "Epoch 2/10, Loss: 0.4891, Accuracy: 0.7602\n",
      "Epoch 3/10, Loss: 0.4823, Accuracy: 0.7644\n",
      "Epoch 4/10, Loss: 0.4774, Accuracy: 0.7678\n",
      "Epoch 5/10, Loss: 0.4735, Accuracy: 0.7701\n",
      "Epoch 6/10, Loss: 0.4702, Accuracy: 0.7722\n",
      "Epoch 7/10, Loss: 0.4675, Accuracy: 0.7739\n",
      "Epoch 8/10, Loss: 0.4649, Accuracy: 0.7756\n",
      "Epoch 9/10, Loss: 0.4629, Accuracy: 0.7769\n",
      "Epoch 10/10, Loss: 0.4611, Accuracy: 0.7779\n",
      "Test Accuracy: 0.7599\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class SentimentNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SentimentNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # Input to hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Hidden to output layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x  # Output logits for multi-class classification\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 50\n",
    "hidden_dim = 32\n",
    "output_dim = 3  # Output dimension is 3 for multi-class classification (negative, neutral, positive)\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_data['processed_text']).toarray()\n",
    "X_test_tfidf = vectorizer.transform(test_data['processed_text']).toarray()\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_tfidf, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_tfidf, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(train_data['label'].apply(lambda x: {0: 0, 2: 1, 4: 2}[x]).values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(test_data['label'].apply(lambda x: {0: 0, 2: 1, 4: 2}[x]).values, dtype=torch.long)\n",
    "\n",
    "model = SentimentNN(input_dim=1000, hidden_dim=64, output_dim=3)  # Three output classes\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get predictions and compute accuracy\n",
    "        _, predicted = torch.max(outputs, 1)  # Get class with max probability\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct_predictions = (predicted == y_test_tensor).sum().item()\n",
    "    total_predictions = y_test_tensor.size(0)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(input_text, model, vectorizer):\n",
    "    # Preprocess input (apply same preprocessing as during training)\n",
    "    # If you had specific preprocessing (like lowercasing, removing stopwords, etc.), apply here\n",
    "    processed_text = input_text.lower()  # Example preprocessing step\n",
    "\n",
    "    # Convert text to TF-IDF features using the trained vectorizer\n",
    "    input_tfidf = vectorizer.transform([processed_text]).toarray()\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    input_tensor = torch.tensor(input_tfidf, dtype=torch.float32)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculation since we are not training\n",
    "    with torch.no_grad():\n",
    "        # Pass the input through the model\n",
    "        outputs = model(input_tensor)\n",
    "\n",
    "        # Get predicted class (index of the highest value)\n",
    "        _, predicted_class = torch.max(outputs, 1)\n",
    "\n",
    "        # Map predicted class back to original label\n",
    "        label_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "        predicted_label = label_mapping[predicted_class.item()]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage\n",
    "input_text = \"The weather is average today.\"\n",
    "predicted_sentiment = predict_sentiment(input_text, model, vectorizer)\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
