{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/kazanova/sentiment140?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80.9M/80.9M [00:01<00:00, 43.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/marius/.cache/kagglehub/datasets/kazanova/sentiment140/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/marius/.cache/kagglehub/datasets/kazanova/sentiment140/versions/2\n",
      "Files in dataset directory: ['training.1600000.processed.noemoticon.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(\"Files in dataset directory:\", os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = os.path.join(path, \"training.1600000.processed.noemoticon.csv\")\n",
    "\n",
    "columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "full_data = pd.read_csv(file_path, encoding='latin-1', names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1      0  is upset that he can't update his Facebook by ...\n",
      "2      0  @Kenichan I dived many times for the ball. Man...\n",
      "3      0    my whole body feels itchy and like its on fire \n",
      "4      0  @nationwideclass no, it's not behaving at all....\n",
      "5      0                      @Kwesidei not the whole crew \n",
      "6      0                                        Need a hug \n",
      "7      0  @LOLTrish hey  long time no see! Yes.. Rains a...\n",
      "8      0               @Tatiana_K nope they didn't have it \n",
      "9      0                          @twittera que me muera ? \n"
     ]
    }
   ],
   "source": [
    "#we are interested in the labels and tweets only\n",
    "\n",
    "data = full_data[['target', 'text']].copy()\n",
    "data.rename(columns={\"target\": \"label\"}, inplace=True)\n",
    "\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                     processed_text\n",
      "0      0  switchfoot http twitpiccomyzl awww bummer shou...\n",
      "1      0  upset ca nt update facebook texting might cry ...\n",
      "2      0  kenichan dived many time ball managed save res...\n",
      "3      0                    whole body feel itchy like fire\n",
      "4      0             nationwideclass behaving mad ca nt see\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function without parallelization (on my hardware took ~3min)\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
    "\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    tokens = [re.sub(r\"[^a-zA-Z]\", \"\", token) for token in tokens]\n",
    "    tokens = [token for token in tokens if token]  # Remove empty strings\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens) \n",
    "\n",
    "data['processed_text'] = data['text'].apply(preprocess_text)\n",
    "new_data = data[['label', 'processed_text']]\n",
    "print(new_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                     processed_text\n",
      "0      0  switchfoot http twitpiccomyzl awww bummer shou...\n",
      "1      0  upset ca nt update facebook texting might cry ...\n",
      "2      0  kenichan dived many time ball managed save res...\n",
      "3      0                    whole body feel itchy like fire\n",
      "4      0             nationwideclass behaving mad ca nt see\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function with parallelization (on my hardware took ~2min)\n",
    "# Roughly a 33% speed increase\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import re\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
    "\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    tokens = [re.sub(r\"[^a-zA-Z]\", \"\", token) for token in tokens]\n",
    "    tokens = [token for token in tokens if token]  # Remove empty strings\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)  # Return processed text as a single string\n",
    "\n",
    "def preprocess_and_return(row):\n",
    "    return row['label'], preprocess_text(row['text'])\n",
    "\n",
    "processed_data = Parallel(n_jobs=-1)(delayed(preprocess_and_return)(row) for index, row in data.iterrows())\n",
    "new_data = pd.DataFrame(processed_data, columns=['label', 'processed_text'])\n",
    "print(new_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
