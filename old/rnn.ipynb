{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.9941961491694216\n",
      "Epoch [2/10], Loss: 0.9306752974869775\n",
      "Epoch [3/10], Loss: 0.9266952764792521\n",
      "Epoch [4/10], Loss: 0.9034421805475579\n",
      "Epoch [5/10], Loss: 0.8408381714195502\n",
      "Epoch [6/10], Loss: 0.807283992650079\n",
      "Epoch [7/10], Loss: 0.7873260808772728\n",
      "Epoch [8/10], Loss: 0.7887290217837349\n",
      "Epoch [9/10], Loss: 0.7723253476815145\n",
      "Epoch [10/10], Loss: 0.7643671915179393\n",
      "Test Accuracy: 65.87628865979381%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Load GloVe embeddings (adjust the file path as needed)\n",
    "glove_embeddings = load_glove_embeddings(\"glove/glove.6B.100d.txt\")  # Adjust path to the GloVe file\n",
    "\n",
    "# Preprocess text with stopword removal and lemmatization using spaCy\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    # Remove stopwords and non-alphabetic words, and return lemmatized words\n",
    "    return [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "\n",
    "# Load dataset (Kaggle dataset)\n",
    "path = \"/Users/marius/.cache/kagglehub/datasets/ankurzing/sentiment-analysis-for-financial-news/versions/5\"  # Adjust path\n",
    "file_path = os.path.join(path, \"all-data.csv\")\n",
    "columns = [\"Sentiment\", \"News Headline\"]\n",
    "df = pd.read_csv(file_path, encoding='latin-1', names=columns)\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={\"Sentiment\": \"label\", \"News Headline\": \"text\"}, inplace=True)\n",
    "\n",
    "# Preprocess text\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to get GloVe embedding for a word, return zeros if word is not in GloVe\n",
    "def get_glove_embedding(word):\n",
    "    return glove_embeddings.get(word, np.zeros(100))  # Assuming 100-dimensional GloVe embeddings\n",
    "\n",
    "# Encode text to GloVe embeddings\n",
    "def encode_phrase_with_glove(phrase):\n",
    "    return [get_glove_embedding(word) for word in phrase]\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(encode_phrase_with_glove)\n",
    "test_data['text'] = test_data['text'].apply(encode_phrase_with_glove)\n",
    "\n",
    "# Padding sequences of GloVe embeddings\n",
    "def pad_sequence_embeddings(seq, max_length):\n",
    "    return seq + [np.zeros(100)] * (max_length - len(seq))  # Padding with zeros to match max_length\n",
    "\n",
    "# Calculate max length of sequences\n",
    "max_length = max(df['text'].apply(len))\n",
    "\n",
    "# Apply padding\n",
    "train_data['text'] = train_data['text'].apply(lambda x: pad_sequence_embeddings(x, max_length))\n",
    "test_data['text'] = test_data['text'].apply(lambda x: pad_sequence_embeddings(x, max_length))\n",
    "\n",
    "# Convert data into PyTorch tensors\n",
    "def prepare_data(df, max_length):\n",
    "    # Pad and convert text into tensors\n",
    "    X = np.array(df['text'].tolist())\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    \n",
    "    # Labels\n",
    "    y = torch.tensor(df['label'].values, dtype=torch.long)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Prepare train and test data\n",
    "X_train, y_train = prepare_data(train_data, max_length)\n",
    "X_test, y_test = prepare_data(test_data, max_length)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, max_length):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Define layers\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)  # (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        # RNN Layer\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer to get output\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize model\n",
    "input_dim = 100  # GloVe embedding dimension\n",
    "hidden_dim = 128  # Number of hidden units in the RNN\n",
    "output_dim = 3  # Number of classes (3 in this case)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, output_dim, max_length).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
